---
title: "Practical Machine Learning Course Project"
author: "GaryFH"
date: "May 4, 2017"
output: md_document
---

###Setup

```{r message=FALSE,warning=FALSE}
library(AppliedPredictiveModeling);library(caret);library(rpart);library(pgmm);library(ElemStatLearn)
library(rpart.plot);library(rattle);library(e1071);library(forecast);library(gbm);library(plyr)
library(lubridate);library(dplyr);library(ggplot2);library(lattice);library(randomForest)

```


#OVERVIEW
#### Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

##DOWNLOAD DATA (From working directory)
#####The training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
#####The test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


```{r download}
        traindata<-read.csv("pml-training.csv")
        testdata<-read.csv("pml-testing.csv")
        traina<-tbl_df(traindata)
        testa<-tbl_df(testdata)
        dim(traina)
        dim(testa)
```

#how many different types of classe exist


```{r howmanyclasses}
        trainb<-group_by(traina,classe)
        trainc<-arrange(trainb,classe)
        traind<-summarise(trainc,n_distinct(classe))
        traind[,1]

```


### Remove near zero variables and Remove variable with mostly (95%+) NA's.  Also remove first five columns(id only)

```{r cleanup}
nza<-nearZeroVar(traina)
traine<-traina[,-nza]
testb<-testa[,-nza]
mostNA<-sapply(traine, function(x) mean(is.na(x)))>=.95
trainf<-traine[,mostNA==F]
testc<-testb[,mostNA==F]
Train<-trainf[,-(1:5)]
Test<-testc[,-(1:5)]

dim(Train)
dim(Test)

```


### How many observations of each classe are found?

```{r HowManyEachClass}
        NumA<-nrow( filter(Train,classe=="A"))
        NumA
        NumB<-nrow( filter(Train,classe=="B"))
        NumB
        NumC<-nrow( filter(Train,classe=="C"))
        NumC
        NumD<-nrow( filter(Train,classe=="D"))
        NumD
        NumE<-nrow( filter(Train,classe=="E"))
        NumE
        Totalclasse<-NumA+NumB+NumC+NumD+NumE
        Totalclasse
        
```


### Split main training database into two training datasets

```{r Split}
set.seed(100)
split<-createDataPartition(y=Train$classe,p=.7,list = F)
trainbig<-Train[split,]
trainsmall<-Train[-split,]

dim(trainbig)
dim(trainsmall)

```

## Use machine learning functions to creat a model

### Try Model Building with "Random Forest"

```{r RandomForest}
set.seed(200)
control<-trainControl(method = "cv",number = 3,verboseIter = FALSE)

RFmodelbig<-train(classe~.,data=trainbig,method="rf",trControl=control)
RFmodelbig$finalModel
```

### Assess accuracy of RF model

```{r RFassessment}
RFpred<-predict(RFmodelbig,newdata = trainsmall)
RF<-confusionMatrix(trainsmall$classe,RFpred)
RF


```

### Try Model Building with "Generalized Boosted Model"(GBM) and assess accuracy GBM model

```{r }
set.seed(200)
GBMmodelbig<-train(classe~.,data=trainbig,method="gbm",trControl=control,verbose=FALSE)
GBMpred<-predict(GBMmodelbig,newdata = trainsmall)
GBM<-confusionMatrix(trainsmall$classe,GBMpred)
GBMmodelbig$finalModel
GBM

```

### Both RF and GBM had good accuracy - RF was the better of the two methods and therefore used RF (99.8% accurate) to predict the classe for each of the twenty observations in the "test" database.

```{r prediction}
FinalPred<-predict(RFmodelbig,newdata = Test)
FinalPreddf<-data.frame(Test_ID=Test$problem_id,PredictClass=FinalPred)
FinalPreddf
```

## Conclusion - the Random Forest Method of model building proved itself very accurate (over 99%) for this project.    Combining multiple models was not considered essential since the RF method proved itself so accurate.


